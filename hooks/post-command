#!/bin/bash
set -euo pipefail
DIR="$(cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd)"

# shellcheck source=lib/shared.bash
. "$DIR/../lib/shared.bash"

if [[ $BUILDKITE_COMMAND_EXIT_STATUS -ne 0 ]]; then
  echo "Command failed, not uploading cache"
  exit 0
fi

cache_key=$(get_cache_key)
bucket="${BUILDKITE_PLUGIN_CACHE_S3_BUCKET_NAME}/${BUILDKITE_ORGANIZATION_SLUG}/${BUILDKITE_PIPELINE_SLUG}/$cache_key"

paths=()
while IFS='=' read -r path _ ; do
  if [[ $path =~ ^(BUILDKITE_PLUGIN_CACHE_PATHS_[0-9]+) ]] ; then
    paths+=("${!path}")
  fi
done < <(env | sort | grep BUILDKITE_PLUGIN_CACHE_PATHS_)

tmpdir=$(mktemp -d)
if [[ "${BUILDKITE_PLUGIN_CACHE_RESTORED:-}" == *":$cache_key:"* ]]; then
  echo "Not uploading new cache - it was restored"
  exit 0
fi

echo "Compressing cache for $cache_key"
tar -czf "$tmpdir/cache.tar.gz" "${paths[@]}"

echo "Saving cache $cache_key"
aws s3 cp "$tmpdir/cache.tar.gz" "s3://${bucket}/cache.tar.gz"
rm -rf "$tmpdir"
